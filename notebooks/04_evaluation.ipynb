{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "This notebook provides comprehensive evaluation of all trained models:\n",
    "- Baseline models (Logistic Regression, Random Forest, Isolation Forest, LOF)\n",
    "- Sequential models (LSTM, TCN, Autoencoder)\n",
    "\n",
    "It includes:\n",
    "- ROC curves comparison\n",
    "- Score distributions\n",
    "- Comprehensive metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "except OSError:\n",
    "    try:\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "    except OSError:\n",
    "        plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# import project modules\n",
    "from src.data.preprocess import load_and_preprocess\n",
    "from src.data.sequence_preparation import prepare_sequences_for_training\n",
    "from src.data.dataset import create_dataloaders_from_dict\n",
    "from src.features.temporal_features import extract_temporal_features\n",
    "from src.models.baselines import load_baseline_model\n",
    "from src.models.lstm import LSTMModel\n",
    "from src.models.tcn import TCNModel\n",
    "from src.models.autoencoder import AutoencoderModel\n",
    "from src.training.evaluate import (\n",
    "    evaluate_sequential_model,\n",
    "    compare_all_models,\n",
    "    plot_roc_curve,\n",
    "    plot_score_distributions,\n",
    "    compute_metrics,\n",
    ")\n",
    "from src.utils.config import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models\n",
    "\n",
    "Load test data and all trained models for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "config = load_config(project_root / \"config\" / \"config.yaml\")\n",
    "\n",
    "# load data\n",
    "data_path = project_root / \"data\" / \"raw\" / \"engagement_timeseries.parquet\"\n",
    "df = load_and_preprocess(\n",
    "    file_path=str(data_path),\n",
    "    target_timezone=\"UTC\",\n",
    "    resample_frequency=\"h\",\n",
    "    handle_missing=True,\n",
    "    missing_method=\"forward\",\n",
    "    normalize=False\n",
    ")\n",
    "\n",
    "# prepare features for baseline models\n",
    "features_df = extract_temporal_features(df, aggregate_per_id=True)\n",
    "\n",
    "# prepare sequences for sequential models\n",
    "sequences_dict = prepare_sequences_for_training(\n",
    "    df,\n",
    "    seq_len=config[\"data\"][\"seq_len\"],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# create test dataloader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders = create_dataloaders_from_dict(\n",
    "    sequences_dict,\n",
    "    batch_size=config[\"training\"][\"batch_size\"],\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_seed=42\n",
    ")\n",
    "test_loader = dataloaders[\"test\"]\n",
    "\n",
    "print(\"Data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline models\n",
    "baseline_results = {}\n",
    "baseline_dir = project_root / \"models\" / \"baselines\"\n",
    "\n",
    "for model_type in [\"logistic_regression\", \"random_forest\", \"isolation_forest\"]:\n",
    "    model_path = baseline_dir / f\"{model_type}.pkl\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            model = load_baseline_model(str(model_path))\n",
    "            from src.training.train import prepare_data\n",
    "            X_train, X_test, y_train, y_test, _ = prepare_data(features_df, test_size=0.2, random_state=42)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            baseline_results[model_type] = (model, X_test, y_test, y_pred, y_proba)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Metrics Table\n",
    "\n",
    "Create comparison table of all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    metrics = compute_metrics(y_test, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"baseline\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    metrics = compute_metrics(y_true, y_pred, y_proba)\n",
    "    metrics[\"model_type\"] = \"sequential\"\n",
    "    all_metrics[model_name] = metrics\n",
    "\n",
    "# create DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics).T\n",
    "display_metrics = [\"auc\", \"precision\", \"recall\", \"f1\", \"false_positive_rate\"]\n",
    "metrics_display = metrics_df[display_metrics].copy()\n",
    "metrics_display[\"model_type\"] = metrics_df[\"model_type\"]\n",
    "metrics_display = metrics_display.sort_values(\"auc\", ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nMetrics Table:\")\n",
    "print(metrics_display.round(4))\n",
    "\n",
    "# best model\n",
    "best_model_name = metrics_display.index[0]\n",
    "best_metrics = metrics_display.loc[best_model_name]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"BEST MODEL: {best_model_name.upper()}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  AUC: {best_metrics['auc']:.4f}\")\n",
    "print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions\n",
    "\n",
    "Final summary of evaluation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# initialize variables if not defined\n",
    "if 'baseline_results' not in locals():\n",
    "    baseline_results = {}\n",
    "if 'sequential_results' not in locals():\n",
    "    sequential_results = {}\n",
    "if 'all_metrics' not in locals():\n",
    "    all_metrics = {}\n",
    "\n",
    "if len(all_metrics) > 0:\n",
    "    print(f\"\\nTotal models evaluated: {len(all_metrics)}\")\n",
    "    print(f\"  - Baseline models: {len(baseline_results)}\")\n",
    "    print(f\"  - Sequential models: {len(sequential_results)}\")\n",
    "else:\n",
    "    print(\"\\nNo models evaluated yet. Please run the metrics computation cell first.\")\n",
    "\n",
    "if ('all_metrics' in locals() and len(all_metrics) > 0 and \n",
    "    'best_model_name' in locals() and 'best_metrics' in locals()):\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    print(f\"  - AUC: {best_metrics['auc']:.4f}\")\n",
    "    print(f\"  - Precision: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"  - F1-Score: {best_metrics['f1']:.4f}\")\n",
    "    print(f\"  - False Positive Rate: {best_metrics['false_positive_rate']:.4f}\")\n",
    "    \n",
    "    # model type comparison\n",
    "    if 'metrics_display' in locals() and 'display_metrics' in locals():\n",
    "        baseline_models = metrics_display[metrics_display['model_type'] == 'baseline']\n",
    "        sequential_models = metrics_display[metrics_display['model_type'] == 'sequential']\n",
    "        \n",
    "        if len(baseline_models) > 0 and len(sequential_models) > 0:\n",
    "            baseline_avg = baseline_models[display_metrics].mean()\n",
    "            sequential_avg = sequential_models[display_metrics].mean()\n",
    "            \n",
    "            print(f\"\\nAverage Performance by Model Type:\")\n",
    "            print(f\"  Baseline models:\")\n",
    "            for metric in display_metrics:\n",
    "                print(f\"    {metric}: {baseline_avg[metric]:.4f}\")\n",
    "            print(f\"  Sequential models:\")\n",
    "            for metric in display_metrics:\n",
    "                print(f\"    {metric}: {sequential_avg[metric]:.4f}\")\n",
    "            \n",
    "            improvement = sequential_avg['auc'] - baseline_avg['auc']\n",
    "            print(f\"\\n  Improvement (Sequential vs Baseline):\")\n",
    "            print(f\"    AUC: {improvement:.4f} ({improvement / baseline_avg['auc'] * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sequential models\n",
    "sequential_results = {}\n",
    "sequential_dir = project_root / \"models\" / \"sequential\"\n",
    "\n",
    "for model_type in [\"lstm\", \"tcn\", \"autoencoder\"]:\n",
    "    model_path = sequential_dir / f\"{model_type}_best.pth\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(str(model_path), map_location=device, weights_only=False)\n",
    "            if model_type == \"lstm\":\n",
    "                model = LSTMModel(**config[\"models\"][\"lstm\"])\n",
    "            elif model_type == \"tcn\":\n",
    "                model = TCNModel(**config[\"models\"][\"tcn\"])\n",
    "            elif model_type == \"autoencoder\":\n",
    "                model = AutoencoderModel(**config[\"models\"][\"autoencoder\"], seq_len=config[\"data\"][\"seq_len\"])\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            sequential_results[model_type] = (model, test_loader, device, model_type)\n",
    "            print(f\"Loaded {model_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_type}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC Curves\n",
    "\n",
    "Plot ROC curves for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# baseline models\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    plot_roc_curve(y_test, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "# sequential models\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    plot_roc_curve(y_true, y_proba, model_name=model_name, ax=ax)\n",
    "\n",
    "ax.set_title(\"ROC Curves - All Models\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Distributions\n",
    "\n",
    "Visualize score distributions for normal vs fake classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect predictions\n",
    "all_predictions = {}\n",
    "\n",
    "for model_name, (model, X_test, y_test, y_pred, y_proba) in baseline_results.items():\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_test, y_proba_positive)\n",
    "\n",
    "for model_name, (model, dataloader, device, model_type) in sequential_results.items():\n",
    "    y_true, y_pred, y_proba = evaluate_sequential_model(model, dataloader, device, model_type)\n",
    "    if y_proba.ndim > 1:\n",
    "        y_proba_positive = y_proba[:, 1] if y_proba.shape[1] > 1 else y_proba.flatten()\n",
    "    else:\n",
    "        y_proba_positive = y_proba\n",
    "    all_predictions[model_name] = (y_true, y_proba_positive)\n",
    "\n",
    "# plot distributions\n",
    "n_models = len(all_predictions)\n",
    "fig, axes = plt.subplots((n_models + 1) // 2, 2, figsize=(16, 4 * ((n_models + 1) // 2)))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, (y_true, y_proba)) in enumerate(all_predictions.items()):\n",
    "    ax = axes[idx]\n",
    "    normal_scores = y_proba[y_true == 0]\n",
    "    fake_scores = y_proba[y_true == 1]\n",
    "    ax.hist(normal_scores, bins=50, alpha=0.6, label=\"Normal\", color=\"blue\", density=True, histtype=\"step\", linewidth=2)\n",
    "    ax.hist(fake_scores, bins=50, alpha=0.6, label=\"Fake\", color=\"red\", density=True, histtype=\"step\", linewidth=2, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Prediction Score\", fontsize=12)\n",
    "    ax.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax.set_title(f\"Score Distribution - {model_name.upper()}\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics Heatmap Comparison\n",
    "\n",
    "Create a comprehensive heatmap comparing all models across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create comprehensive metrics heatmap\n",
    "if ('all_metrics' in locals() and 'metrics_display' in locals() and \n",
    "    'display_metrics' in locals() and len(all_metrics) > 0):\n",
    "    # prepare data for heatmap\n",
    "    heatmap_data = metrics_display[display_metrics].T\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(max(10, len(all_metrics) * 1.5), 6))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Score'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Model Performance Heatmap - All Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Metric', fontsize=12)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # create comparison bar chart\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(display_metrics))\n",
    "    width = 0.8 / len(metrics_display)\n",
    "    \n",
    "    colors_map = {'baseline': 'blue', 'sequential': 'red'}\n",
    "    for idx, (model_name, row) in enumerate(metrics_display.iterrows()):\n",
    "        values = [row[m] for m in display_metrics]\n",
    "        color = colors_map.get(row['model_type'], 'gray')\n",
    "        ax.bar(x + idx * width, values, width, label=model_name, \n",
    "               alpha=0.7, color=color, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Metric', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Model Performance Comparison - All Metrics', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(x + width * (len(metrics_display) - 1) / 2)\n",
    "    ax.set_xticklabels(display_metrics)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models loaded. Please load models first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
